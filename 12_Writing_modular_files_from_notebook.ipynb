{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4865d64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pranshumaan'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b1c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT\n"
     ]
    }
   ],
   "source": [
    "cd /media/pranshumaan/TOSHIBA\\ EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d204a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev\n"
     ]
    }
   ],
   "source": [
    "cd Dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29c3f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mDeep_Learning_Pytorch_Udemy_Tutorial\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9739a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial\n"
     ]
    }
   ],
   "source": [
    "cd Deep_Learning_Pytorch_Udemy_Tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838e2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Linear_Regression_1.ipynb\r\n",
      "02_Linear_Regression_2.ipynb\r\n",
      "03_Neural_network_classification.ipynb\r\n",
      "04_Neural_network_classification.ipynb\r\n",
      "05_Neural_network_multilabel_classification.ipynb\r\n",
      "06_Neural_network_multilabel_classification.ipynb\r\n",
      "07_Computer_vision_and_CNN.ipynb\r\n",
      "08_Computer_vision_and_CNN.ipynb\r\n",
      "09_Pytorch_custom_datasets.ipynb\r\n",
      "10_Pytorch_custom_datasets.ipynb\r\n",
      "11_Pytorch_custom_datasets.ipynb\r\n",
      "\u001b[0m\u001b[01;34mCourse_creator_notebooks\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/\r\n",
      "\u001b[01;34mdata_pizza_steak_sushi\u001b[0m/\r\n",
      "\u001b[01;34mmodels\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d86b75",
   "metadata": {},
   "source": [
    "#### Get data\n",
    "Assumed that the data already exists in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede447c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bde9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir TinyVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63253c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial/TinyVGG\n"
     ]
    }
   ],
   "source": [
    "cd TinyVGG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3e2a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial/TinyVGG'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d0d5d",
   "metadata": {},
   "source": [
    "#### Dataloader Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b52f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup.py\n",
    "\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "    \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "    Takes in a training directory and testing directory path and turns\n",
    "    them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "    \"\"\"\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eeb1b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial/TinyVGG'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fc0c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a4f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "594c2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'pizza_steak_sushi/train'\n",
    "test_dir = 'pizza_steak_sushi/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f426130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_transform = transforms.Compose([transforms.Resize(size=(64,64)),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "301d6264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f47a1aa8160>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f47a187bc10>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=data_transform,\n",
    "                                                                               batch_size=32)\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c599670",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40500359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_builder.py\n",
    "\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a TinyVGG model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"Creates the TinyVGG architecture.\n",
    "\n",
    "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "    Args:\n",
    "    input_shape: An integer indicating number of input channels.\n",
    "    hidden_units: An integer indicating number of hidden units between layers.\n",
    "    output_shape: An integer indicating number of output units.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=input_shape, \n",
    "                    out_channels=hidden_units, \n",
    "                    kernel_size=3, \n",
    "                    stride=1, \n",
    "                    padding=0),  \n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=hidden_units, \n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          # Where did this in_features shape come from? \n",
    "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "          nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "045e8ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import model_builder\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_2 = model_builder.TinyVGG(3,10,3)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11721ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pranshumaan'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935ab93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT\n"
     ]
    }
   ],
   "source": [
    "cd /media/pranshumaan/TOSHIBA\\ EXT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7734c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial\n"
     ]
    }
   ],
   "source": [
    "cd Dev/Deep_Learning_Pytorch_Udemy_Tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf419ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f9b022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_Linear_Regression_1.ipynb\r\n",
      "02_Linear_Regression_2.ipynb\r\n",
      "03_Neural_network_classification.ipynb\r\n",
      "04_Neural_network_classification.ipynb\r\n",
      "05_Neural_network_multilabel_classification.ipynb\r\n",
      "06_Neural_network_multilabel_classification.ipynb\r\n",
      "07_Computer_vision_and_CNN.ipynb\r\n",
      "08_Computer_vision_and_CNN.ipynb\r\n",
      "09_Pytorch_custom_datasets.ipynb\r\n",
      "10_Pytorch_custom_datasets.ipynb\r\n",
      "11_Pytorch_custom_datasets.ipynb\r\n",
      "\u001b[0m\u001b[01;34mCourse_creator_notebooks\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/\r\n",
      "\u001b[01;34mdata_pizza_steak_sushi\u001b[0m/\r\n",
      "\u001b[01;34mmodels\u001b[0m/\r\n",
      "\u001b[01;34mTinyVGG\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b926cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev/Deep_Learning_Pytorch_Udemy_Tutorial/TinyVGG\n"
     ]
    }
   ],
   "source": [
    "cd TinyVGG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41885629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  model_builder.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f20433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile engine.py\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb4115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  engine.py  model_builder.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f02c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "    Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "    \"\"\"\n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef4ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  model_builder.py    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\r\n",
      "engine.py      \u001b[01;34mpizza_steak_sushi\u001b[0m/  utils.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c19334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "\"\"\"\n",
    "Trains a PyTorch image classification model using device-agnostic code.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Setup directories\n",
    "train_dir = \"pizza_steak_sushi/train\"\n",
    "test_dir = \"pizza_steak_sushi/test\"\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create transforms\n",
    "data_transform = transforms.Compose([\n",
    "  transforms.Resize((64, 64)),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create DataLoaders with help from data_setup.py\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create model with help from model_builder.py\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Set loss and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "\n",
    "# Start training with help from engine.py\n",
    "engine.train(model=model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader,\n",
    "             loss_fn=loss_fn,\n",
    "             optimizer=optimizer,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             device=device)\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "utils.save_model(model=model,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56fdf744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  model_builder.py    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  utils.py\r\n",
      "engine.py      \u001b[01;34mpizza_steak_sushi\u001b[0m/  train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8d4d017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1036 | train_acc: 0.2773 | test_loss: 1.0944 | test_acc: 0.3106\n",
      " 20%|█████████                                    | 1/5 [00:02<00:09,  2.29s/it]Epoch: 2 | train_loss: 1.1035 | train_acc: 0.2969 | test_loss: 1.1086 | test_acc: 0.2812\n",
      " 40%|██████████████████                           | 2/5 [00:03<00:04,  1.58s/it]Epoch: 3 | train_loss: 1.0737 | train_acc: 0.5273 | test_loss: 1.0788 | test_acc: 0.2812\n",
      " 60%|███████████████████████████                  | 3/5 [00:04<00:02,  1.37s/it]Epoch: 4 | train_loss: 1.0739 | train_acc: 0.3672 | test_loss: 1.0702 | test_acc: 0.2812\n",
      " 80%|████████████████████████████████████         | 4/5 [00:05<00:01,  1.33s/it]Epoch: 5 | train_loss: 1.0742 | train_acc: 0.3906 | test_loss: 1.0633 | test_acc: 0.4233\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:06<00:00,  1.35s/it]\n",
      "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2c8953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  model_builder.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  train.py\r\n",
      "engine.py      \u001b[01;34mmodels\u001b[0m/           \u001b[01;34m__pycache__\u001b[0m/        utils.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a1689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "data_setup.py  model_builder.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  train.py\n",
      "engine.py      \u001b[01;34mmodels\u001b[0m/           \u001b[01;34m__pycache__\u001b[0m/        utils.py\n",
      "\n",
      "./models:\n",
      "05_going_modular_script_mode_tinyvgg_model.pth\n",
      "\n",
      "./pizza_steak_sushi:\n",
      "\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n",
      "\n",
      "./pizza_steak_sushi/test:\n",
      "\u001b[01;34mpizza\u001b[0m/  \u001b[01;34msteak\u001b[0m/  \u001b[01;34msushi\u001b[0m/\n",
      "\n",
      "./pizza_steak_sushi/test/pizza:\n",
      "1152100.jpg  195160.jpg   2218680.jpg  309892.jpg   540882.jpg\n",
      "1503858.jpg  2003290.jpg  2236338.jpg  344397.jpg   648055.jpg\n",
      "1687143.jpg  2019408.jpg  2508636.jpg  3475871.jpg  714866.jpg\n",
      "1925494.jpg  2111981.jpg  2871261.jpg  398345.jpg   930553.jpg\n",
      "194643.jpg   2124579.jpg  3092704.jpg  416067.jpg   971934.jpg\n",
      "\n",
      "./pizza_steak_sushi/test/steak:\n",
      "100274.jpg   1627703.jpg  2144308.jpg  354513.jpg   690177.jpg\n",
      "1016217.jpg  1868005.jpg  27415.jpg    3873283.jpg  894825.jpg\n",
      "1285886.jpg  1882831.jpg  296375.jpg   502076.jpg   966174.jpg\n",
      "1302563.jpg  2117351.jpg  3424937.jpg  673127.jpg\n",
      "\n",
      "./pizza_steak_sushi/test/sushi:\n",
      "1172255.jpg  1742201.jpg  2385731.jpg  3177743.jpg  479711.jpg  988559.jpg\n",
      "1230335.jpg  175783.jpg   2394442.jpg  3196729.jpg  499605.jpg\n",
      "1245193.jpg  1987407.jpg  2521706.jpg  343036.jpg   684266.jpg\n",
      "1434806.jpg  207578.jpg   2540511.jpg  3806282.jpg  719108.jpg\n",
      "1600999.jpg  2190404.jpg  2715127.jpg  3837522.jpg  858157.jpg\n",
      "1680893.jpg  2276986.jpg  2741434.jpg  46797.jpg    887831.jpg\n",
      "\n",
      "./pizza_steak_sushi/train:\n",
      "\u001b[01;34mpizza\u001b[0m/  \u001b[01;34msteak\u001b[0m/  \u001b[01;34msushi\u001b[0m/\n",
      "\n",
      "./pizza_steak_sushi/train/pizza:\n",
      "1008844.jpg  1654444.jpg  2291093.jpg  2785084.jpg  320570.jpg   5764.jpg\n",
      "1033251.jpg  1660415.jpg  2330965.jpg  2800325.jpg  3269634.jpg  618348.jpg\n",
      "1044789.jpg  1899785.jpg  2382016.jpg  2811032.jpg  3281494.jpg  667309.jpg\n",
      "1089334.jpg  1947572.jpg  2426686.jpg  2821048.jpg  3338774.jpg  68684.jpg\n",
      "1105700.jpg  1968947.jpg  2428085.jpg  2885050.jpg  3441394.jpg  702165.jpg\n",
      "12301.jpg    2026009.jpg  244505.jpg   2885796.jpg  3505182.jpg  715169.jpg\n",
      "1285298.jpg  2121603.jpg  2451169.jpg  2924941.jpg  3530210.jpg  739735.jpg\n",
      "138855.jpg   2154394.jpg  2493954.jpg  29417.jpg    3589437.jpg  741883.jpg\n",
      "1412034.jpg  218711.jpg   2569760.jpg  2992084.jpg  3699992.jpg  764429.jpg\n",
      "1524655.jpg  2190018.jpg  2576168.jpg  300869.jpg   3821701.jpg  765799.jpg\n",
      "1572608.jpg  220190.jpg   2687575.jpg  3018077.jpg  38349.jpg    786995.jpg\n",
      "1633289.jpg  2228322.jpg  2702825.jpg  3109486.jpg  3860002.jpg  853441.jpg\n",
      "1649276.jpg  2285942.jpg  2760984.jpg  3196721.jpg  393658.jpg   928670.jpg\n",
      "\n",
      "./pizza_steak_sushi/train/steak:\n",
      "100135.jpg   1736968.jpg  2129685.jpg  2603058.jpg  3074367.jpg  461689.jpg\n",
      "1225762.jpg  1761285.jpg  214320.jpg   2606444.jpg  3142045.jpg  482022.jpg\n",
      "1257104.jpg  176508.jpg   2163079.jpg  2614189.jpg  3142674.jpg  560503.jpg\n",
      "1264154.jpg  1787505.jpg  2222018.jpg  2614649.jpg  3245622.jpg  735441.jpg\n",
      "1382427.jpg  1839481.jpg  2254705.jpg  2628106.jpg  339891.jpg   75537.jpg\n",
      "1413972.jpg  1849463.jpg  225990.jpg   2629750.jpg  3518960.jpg  830007.jpg\n",
      "1598345.jpg  1937872.jpg  2287136.jpg  2648423.jpg  355715.jpg   914570.jpg\n",
      "1615395.jpg  1961025.jpg  231296.jpg   2707522.jpg  3577618.jpg  922752.jpg\n",
      "1621763.jpg  1966300.jpg  2324994.jpg  2825100.jpg  3727036.jpg  937133.jpg\n",
      "1624747.jpg  1976160.jpg  234626.jpg   2878151.jpg  3727491.jpg  97656.jpg\n",
      "1647351.jpg  2013535.jpg  239025.jpg   2880035.jpg  3857508.jpg\n",
      "165639.jpg   2017387.jpg  2561199.jpg  2881783.jpg  421476.jpg\n",
      "167069.jpg   2087958.jpg  256592.jpg   2979061.jpg  443210.jpg\n",
      "\n",
      "./pizza_steak_sushi/train/sushi:\n",
      "1070104.jpg  1552504.jpg  2021381.jpg  2720223.jpg  3360207.jpg  700405.jpg\n",
      "1129338.jpg  1571146.jpg  2021685.jpg  2797464.jpg  3360232.jpg  710379.jpg\n",
      "1138695.jpg  1575445.jpg  2120573.jpg  2813454.jpg  3426958.jpg  748830.jpg\n",
      "1209865.jpg  1615453.jpg  2175561.jpg  2871052.jpg  3579071.jpg  765684.jpg\n",
      "1214108.jpg  169392.jpg   2267190.jpg  2873571.jpg  3737197.jpg  773725.jpg\n",
      "121940.jpg   170385.jpg   2323548.jpg  2980779.jpg  377047.jpg   794647.jpg\n",
      "1221830.jpg  17704.jpg    2492146.jpg  3004029.jpg  385154.jpg   821108.jpg\n",
      "1232045.jpg  1957449.jpg  2574453.jpg  307738.jpg   390178.jpg   840444.jpg\n",
      "1280119.jpg  200025.jpg   2590819.jpg  3081701.jpg  424994.jpg   843815.jpg\n",
      "14046.jpg    2004525.jpg  2641778.jpg  3107839.jpg  497686.jpg   855721.jpg\n",
      "148799.jpg   2017378.jpg  2674024.jpg  3251688.jpg  542188.jpg   929471.jpg\n",
      "1551817.jpg  2019344.jpg  268990.jpg   3353428.jpg  686426.jpg   93139.jpg\n",
      "\n",
      "./__pycache__:\n",
      "data_setup.cpython-310.pyc  model_builder.cpython-310.pyc\n",
      "engine.cpython-310.pyc      utils.cpython-310.pyc\n"
     ]
    }
   ],
   "source": [
    "ls -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0a9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictions.py\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to make predictions.\n",
    "\n",
    "Main reference for code creation: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set \n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Predict on a target image with a target model\n",
    "# Function created in: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n",
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    class_names: List[str],\n",
    "    image_path: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    \"\"\"Predicts on a target image with a target model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n",
    "        class_names (List[str]): A list of target classes to map predictions to.\n",
    "        image_path (str): Filepath to target image to predict on.\n",
    "        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n",
    "        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n",
    "        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ### Predict on image ###\n",
    "\n",
    "    # Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # Plot image with predicted label and probability\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\n",
    "        f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
    "    )\n",
    "    plt.axis(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8680ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_setup.py  model_builder.py  \u001b[0m\u001b[01;34mpizza_steak_sushi\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  utils.py\r\n",
      "engine.py      \u001b[01;34mmodels\u001b[0m/           predictions.py      train.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083fec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
